[experiment]
seed=0
folder=experiments/mini-librispeech

[model]
num_encoder_tokens=31
num_encoder_layers=1
num_encoder_hidden=1024
num_decoder_layers=2
num_decoder_hidden=1024
num_joiner_hidden=512
bidirectional=True
tokenizer_training_text_path=lm.txt

[training]
base_path=/home/lugosch/data/mini-librispeech
lr=0.001
lr_period=10
gamma=0.9
batch_size=256
num_epochs=1
validation_period=1

[inference]
beam_width=2
